version: '3.5'
services:
  zookeeper:
    image: wurstmeister/zookeeper
    volumes:
      - zoo-stack-data:/tmp/zookeeper
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "2181:2181"
    networks:
      - kafka-net
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS: 16000
    healthcheck:
      test: echo stat | nc localhost 2181
      interval: 10s
      timeout: 10s
      retries: 3

  kafka1:
    image: wurstmeister/kafka:latest
    volumes:
       - kafka-stack-1-logs:/tmp/kafka-logs
       - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - target: 9092
        published: 9092
        protocol: tcp
        mode: ingress
    networks:
       - kafka-net
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_PORT: 9092
      KAFKA_ADVERTISED_PORT: 9092
      KAFKA_CREATE_TOPICS: "tweetsTopic:1:1,sentVen:1:1,sentUSA:1:1,sentRus:1:1,sentChina:1:1,sentIs:1:1,sentGer:1:1,sentJap:1:1,sentIran:1:1,sentBra:1:1"
      KAFKA_ADVERTISED_HOST_NAME: kafka1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: "PLAINTEXT://kafka1:9092"

  twitter-producer:
    image: kmillanr84/twitter-producer
    networks:
      - kafka-net
    secrets:
      - consumer_key
      - consumer_secret
      - access_key
      - access_secret
    deploy:
      restart_policy:
        condition: on-failure
    environment: 
      CONSUMER_KEY: /run/secrets/consumer_key
      CONSUMER_SECRET: /run/secrets/consumer_secret
      ACCESS_KEY: /run/secrets/access_key
      ACCESS_SECRET: /run/secrets/access_secret
    command: ["dockerize", "-wait", "tcp://kafka1:9092", "-timeout", "30s","/bin/bash", "/produce.sh"]

  spark-master:
    image: kmillanr84/spark-master
    ports:
      - "8888:8888"
      - "7077:7077"
    networks:
      - spark-net
    deploy:
      mode: global
      restart_policy:
        condition: on-failure
    volumes:
      - master-logs:/tmp/master-logs
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
    environment:
      - "SPARK_MASTER_PORT=7077"
      - "SPARK_LOCAL_IP=spark-master"
      - "SPARK_MASTER_WEBUI_PORT=8080"
      - "ENABLE_INIT_DAEMON=false"
    
  spark-worker:
    image: bde2020/spark-worker:2.4.5-hadoop2.7
    ports:
      - "8081:8081"
    networks:
      - spark-net
    deploy:
      mode: global
    volumes:
      - worker-logs:/tmp/worker-logs
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data
    environment:
      - "SPARK_MASTER=spark://spark-master:7077"
      - "SPARK_LOCAL_IP=spark-worker"

  # node-backend:
  #   image: kmillanr84/fractal-backend
  #   ports:
  #     - "44444:44444"
  #     - "33334:33334"
  #   networks:
  #       - kafka-net

networks:
  kafka-net:
  spark-net:

secrets:
  consumer_key:
    external: true
  consumer_secret:
    external: true
  access_key:
    external: true
  access_secret:
    external: true

volumes:
  kafka-stack-1-logs:
  master-logs:
  worker-logs:
  zoo-stack-data: