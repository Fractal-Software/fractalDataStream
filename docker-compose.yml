version: '3.5'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    volumes:
      - zoo-stack-data:/tmp/zookeeper
      - /var/run/docker.sock:/var/run/docker.sock
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
    ports:
      - "2181:2181"
    networks:
      - kafka-net
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka1:
    image: confluentinc/cp-kafka
    depends_on:
      - "zookeeper"
    volumes:
       - kafka-stack-1-logs:/tmp/kafka-logs
       - /var/run/docker.sock:/var/run/docker.sock
       - /etc/localtime:/etc/localtime
       - /etc/timezone:/etc/timezone
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_ADVERTISED_HOST_NAME: kafka1 # docker-machine ip
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
      KAFKA_CREATE_TOPICS: "tweetsTopic:1:1,sentVen:1:1,sentUSA:1:1,sentRus:1:1,sentChina:1:1,sentIs:1:1,sentGer:1:1,sentJap:1:1,sentIran:1:1,sentBra:1:1"
    networks:
       - kafka-net
    deploy:
      mode: global
      restart_policy:
        condition: on-failure

  schema-registry:
    image: confluentinc/cp-schema-registry
    depends_on:
      - zookeeper
      - kafka1
    ports:
      - 8081:8081
    networks:
      - kafka-net
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_LISTENERS: http://localhost:8081

  kafka-connect:
    image: confluentinc/cp-kafka-connect
    depends_on:
      - zookeeper
      - kafka1
      - schema-registry
    ports:
      - "8083:8083"
    networks:
      - kafka-net
    environment:
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_BOOTSTRAP_SERVERS: 'kafka1:29092'
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR,com.mongodb.kafka=DEBUG"
      CONNECT_PLUGIN_PATH: /usr/share/java
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'

  postgres:
    image: postgres
    restart: always
    networks:
      - kafka-net
    secrets:
      - postgres_user
      - postgres_pass
    ports:
      - "5432:5432"
    volumes:
      - postgresql-data:/var/lib/postgresql
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
    environment:
      POSTGRES_USER: root
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: test
      
  twitter-producer:
    image: kmillanr84/twitter-producer
    depends_on:
      - zookeeper
      - kafka1
    networks:
      - kafka-net
    secrets:
      - consumer_key
      - consumer_secret
      - access_key
      - access_secret
    volumes:
      - tweet-logs:/tmp/tweet-logs
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      CONSUMER_KEY: /run/secrets/consumer_key
      CONSUMER_SECRET: /run/secrets/consumer_secret
      ACCESS_KEY: /run/secrets/access_key
      ACCESS_SECRET: /run/secrets/access_secret
    command: ["sh","-c","dockerize -wait tcp://kafka1:9092 -timeout 30s && python twitter-producer.py kafka1:9092"]

  master:
    image: kmillanr84/sparktest
    networks:
      - kafka-net
    expose:
      - 7077
    ports:
      - 4040:4040
      - 6066:6066
      - 7077:7077
    volumes:
      - master-logs:/tmp/master-logs
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
      - ./conf/master:/conf
      - ./data:/tmp/data
    environment:
      MASTER: spark://master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
    command: ["sh","-c","bin/spark-class org.apache.spark.deploy.master.Master -h master"]

  worker:
    image: kmillanr84/sparktest
    networks:
      - kafka-net
    depends_on:
      - master
    expose:
      - 8881
      - 8882
      - 8883
    volumes:
      - worker-logs:/tmp/worker-logs
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
      - ./conf/worker:/conf
      - ./data:/tmp/data
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 10g
      SPARK_PUBLIC_DNS: localhost
    command: ["sh","-c","bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077 && bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.3 --driver-memory 12g --class com.inndata.StructuredStreaming.Kafka --master spark://master:7077 --conf spark.streaming.backpressure.enabled=true --conf spark.streaming.backpressure.initialRate=60 --conf spark.streaming.kafka.maxRatePerPartition=200 --conf spark.streaming.backpressure.pid.minRate=1600 --conf spark.driver.memoryOverhead=8g --conf spark.executor.memoryOverhead=8g kafka-pyspark-streaming.py"]

  # node-backend:
  #   image: kmillanr84/fractal-backend
  #   volumes:
  #     - .:/app/
  #   networks:
  #     - kafka-net
  #   ports:
  #     - 33334:3334
  #     - 44444:4444

networks:
  kafka-net:

secrets:
  consumer_key:
    external: true
  consumer_secret:
    external: true
  access_key:
    external: true
  access_secret:
    external: true
  postgres_user:
    external: true
  postgres_pass:
    external: true

volumes:
  kafka-stack-1-logs:
  tweet-logs:
  master-logs:
  worker-logs:
  zoo-stack-data:
  postgresql-data: