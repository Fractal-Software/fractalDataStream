version: '3.5'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper
    volumes:
      - zoo-stack-data:/tmp/zookeeper
      - /var/run/docker.sock:/var/run/docker.sock
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
    ports:
      - "2181:2181"
    networks:
      - kafka-net
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka
    depends_on:
      - "zookeeper"
    volumes:
       - kafka-stack-1-logs:/tmp/kafka-logs
       - /var/run/docker.sock:/var/run/docker.sock
       - /etc/localtime:/etc/localtime
       - /etc/timezone:/etc/timezone
    ports:
      - "29092:29092"
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ADVERTISED_HOST_NAME: broker # docker-machine ip
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: broker:29092
      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'true'
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    networks:
       - kafka-net
    deploy:
      mode: global
      restart_policy:
        condition: on-failure

  schema-registry:
    image: confluentinc/cp-schema-registry
    depends_on:
      - zookeeper
      - broker
    ports:
      - 8081:8081
    networks:
      - kafka-net
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:2181
      SCHEMA_REGISTRY_LISTENERS: http://schema-registry:8081

  rest-proxy:
    image: confluentinc/cp-kafka-rest
    networks:
      - kafka-net
    depends_on:
      - zookeeper
      - broker
      - schema-registry
    ports:
      - 8082:8082
    environment:
      KAFKA_REST_HOST_NAME: rest-proxy
      KAFKA_REST_BOOTSTRAP_SERVERS: 'broker:29092'
      KAFKA_REST_LISTENERS: "http://rest-proxy:8082"
      KAFKA_REST_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'    
      
  kafka-connect:
    image: confluentinc/cp-kafka-connect
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - zookeeper
      - broker
      - schema-registry
    ports:
      - "8083:8083"
    networks:
      - kafka-net
    environment:
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_LOG4J_APPENDER_STDOUT_LAYOUT_CONVERSIONPATTERN: "[%d] %p %X{connector.context}%m (%c:%L)%n"
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: 'http://schema-registry:8081'
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR,com.mongodb.kafka=DEBUG"
      CONNECT_PLUGIN_PATH: /usr/share/java
      CONNECT_ZOOKEEPER_CONNECT: 'zookeeper:2181'
    # command: 
      # - bash 
      # - -c 
      # - |
      # echo "Installing connector plugins"
      # confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:latest
      # #
      # echo "Dowloading JDBC Drivers"
      # cd /usr/share/confluent-hub-components/confluentinc-kafka-connect-jdbc/lib
      # curl http://maven.xwiki.org/externals/com/oracle/jdbc/ojdbc8/12.2.0.1/ojdbc8-12.2.0.1.jar -o ojdbc8-12.2.0.1.jar
      # #
      # echo "Launching Kafka Connect worker"
      # /etc/confluent/docker/run &
      # #
      # sleep infinity

  twitter-producer:
    image: kmillanr84/twitter-producer
    depends_on:
      - zookeeper
      - broker
    networks:
      - kafka-net
    secrets:
      - consumer_key
      - consumer_secret
      - access_key
      - access_secret
    volumes:
      - tweet-logs:/tmp/tweet-logs
    deploy:
      restart_policy:
        condition: on-failure
    environment:
      CONSUMER_KEY: Ar76P3lFUuvdfjnnh1Sqo4tY2
      CONSUMER_SECRET: rPgsYaX7CAFCjvSJ2tS76z41LezyOYcZ9N4lX1GcL1MrRvNeyW
      ACCESS_KEY: 117212674-pUfA8fdQkhwsyQj6mLLN92M1s2Mp74lTlACyolR7
      ACCESS_SECRET: n6ONU6Se17UwqsVGR8cBbrHk1m5kY28FuqxGhPU8Ck3vh
      # CONSUMER_KEY: /run/secrets/consumer_key
      # CONSUMER_SECRET: /run/secrets/consumer_secret
      # ACCESS_KEY: /run/secrets/access_key
      # ACCESS_SECRET: /run/secrets/access_secret
    command: ["sh","-c","dockerize -wait tcp://broker:29092 -timeout 30s && python twitter-producer.py broker:29092"]

  postgres:
    image: postgres
    restart: always
    networks:
      - kafka-net
    ports:
      - "5432:5432"
    volumes:
      - postgresql-data:/var/lib/postgresql
      - ./database/init.sql:/docker-entrypoint-initdb.d/init.sql
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
    environment:
      POSTGRES_USER: pass
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: pass

  master:
    image: kmillanr84/spark-base
    networks:
      - kafka-net
    depends_on:
      - twitter-producer
    expose:
      - 7077
    ports:
      - 4040:4040
      - 6066:6066
      - 7077:7077
    volumes:
      - master-logs:/tmp/master-logs
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
      - ./conf/master:/conf
      - ./data:/tmp/data
    environment:
      MASTER: spark://master:7077
      SPARK_CONF_DIR: /conf
      SPARK_PUBLIC_DNS: localhost
    command: ["sh","-c","bin/spark-class org.apache.spark.deploy.master.Master -h master"]

  worker:
    image: kmillanr84/spark-base
    networks:
      - kafka-net
    depends_on:
      - master
    expose:
      - 8881
      - 8882
      - 8883
    volumes:
      - worker-logs:/tmp/worker-logs
      - /etc/localtime:/etc/localtime
      - /etc/timezone:/etc/timezone
      - ./conf/worker:/conf
      - ./data:/tmp/data
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 10g
      SPARK_PUBLIC_DNS: localhost
    command: ["sh","-c","bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077"]

  spark-submit:
    image: kmillanr84/spark-base
    networks:
      - kafka-net
    depends_on:
      - master
    expose:
      - 8884
      - 8885
      - 8886
    environment:
      SPARK_CONF_DIR: /conf
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 10g
      SPARK_PUBLIC_DNS: localhost
    command: ["sh","-c","bin/spark-submit \
              --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.3 \
              --driver-memory 12g \
              --class com.inndata.StructuredStreaming.Kafka \
              --master spark://master:7077 \
              --conf spark.streaming.backpressure.enabled=true \
              --conf spark.streaming.backpressure.initialRate=60 \
              --conf spark.streaming.kafka.maxRatePerPartition=200 \
              --conf spark.streaming.backpressure.pid.minRate=1600 \
              --conf spark.driver.memoryOverhead=8g \
              --conf spark.executor.memoryOverhead=8g kafka-pyspark-streaming.py"]

  node-backend:
    image: kmillanr84/fractal-backend
    depends_on:
      - postgres
    networks:
      - kafka-net
    ports:
      - 33334:3334
      - 44444:4444

networks:
  kafka-net:

secrets:
  consumer_key:
    external: true
  consumer_secret:
    external: true
  access_key:
    external: true
  access_secret:
    external: true
  postgres_user:
    external: true
  postgres_pass:
    external: true

volumes:
  kafka-stack-1-logs:
  kafka-connect:
  tweet-logs:
  master-logs:
  worker-logs:
  zoo-stack-data:
  postgresql-data: